# -*- coding: utf-8 -*-
"""Bản sao của LLM cho ae

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19FQiKUfZFR9hcr8ELIUYX7LtH-4Jjcmm
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Cài đặt Unsloth với dependencies đúng cách
# 
# # Bước 1: Cài uv (package manager nhanh hơn pip)
# !pip install --upgrade -qqq uv
# 
# # Bước 2: Get numpy version hiện tại
# try:
#     import numpy
#     get_numpy = f"numpy=={numpy.__version__}"
# except:
#     get_numpy = "numpy"
# 
# # Bước 3: Cài đặt tất cả packages cần thiết
# !uv pip install -qqq \
#     "torch>=2.8.0" \
#     "triton>=3.4.0" \
#     {get_numpy} \
#     torchvision \
#     bitsandbytes \
#     "transformers>=4.55.3" \
#     "unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo" \
#     "unsloth[base] @ git+https://github.com/unslothai/unsloth"
# 
# # Bước 4: Cài triton kernels riêng
# !uv pip install -qqq git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels
# 
# # Bước 5: Fix transformers version (lỗi syntax trong code của bạn)
# !uv pip install -qqq transformers==4.55.4
# 
# print("✅ Cài đặt hoàn tất! Hãy restart runtime trước khi import!")

import unsloth
from unsloth import FastLanguageModel
import torch

max_seq_length = 1024
dtype = None
model_name = "unsloth/gpt-oss-20b"


model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    dtype = dtype, # None for auto detection
    max_seq_length = max_seq_length, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)

import os
import shutil

# Đường dẫn gốc tới 2 file bạn có
src_config = "/content/adapter_config (2).json"
src_model = "/content/adapter_model (2).safetensors"

# Thư mục mới để gom adapter
adapter_dir = "/content/lora_adapter"
os.makedirs(adapter_dir, exist_ok=True)

# Đích đến với tên gọn chuẩn HuggingFace
dst_config = os.path.join(adapter_dir, "adapter_config.json")
dst_model = os.path.join(adapter_dir, "adapter_model.safetensors")

# Copy và đổi tên
shutil.copy(src_config, dst_config)
shutil.copy(src_model, dst_model)

print("✅ Đã tạo thư mục adapter:", adapter_dir)
print("Chứa các file:", os.listdir(adapter_dir))

from peft import PeftModel

# Thư mục chứa adapter_config.json + adapter_model.safetensors
adapter_path = "/content/lora_adapter"

# Load LoRA adapter vào model gốc
model = PeftModel.from_pretrained(
    model,
    adapter_path,
    torch_dtype=torch.float16,
    is_trainable=False,   # inference thôi, không training tiếp
    device_map="auto"     # để tự động map GPU/CPU
)

# Test thử
inputs = tokenizer("Xin chào, tôi đã load LoRA thành công!", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# messages = [
#     {"role": "system", "content": "Bạn là một chuyên gia thẩm định tín dụng có 15 năm kinh nghiệm tại các ngân hàng lớn ở Việt Nam. Nhiệm vụ của bạn là giải thích vì sao hồ sơ vay này nhận được quyết định từ mô hình AI dựa trên các thông tin đầu vào và phân tích SHAP values.\n\nTỶ LỆ HỒ SƠ ĐƯỢC DUYỆT TRUNG BÌNH: 0.782 (78.2%)\n\nHãy phân tích từng yếu tố ảnh hưởng đến quyết định, giải thích tác động tích cực/tiêu cực của chúng, và đưa ra lời khuyên phù hợp cho khách hàng không quá 500 từ, person_income là USD."},
#     {"role": "user", "content": "=== KẾT QUẢ DỰ ĐOÁN CỦA MÔ HÌNH AI ===\n• Quyết định: TỪ CHỐI\n• Xác suất chấp thuận: 0.0270 (2.7%)\n\n=== THÔNG TIN HỒ SƠ VAY ===\n• person_age: 22 tuổi\n• person_income: 44,000 USD/năm\n• person_home_ownership: RENT\n• person_emp_length: 3 năm\n• loan_intent: HOMEIMPROVEMENT\n• loan_amnt: 3,600 USD\n• cb_person_default_on_file: Y\n• cb_person_cred_hist_length: 3 năm\n\n=== PHÂN TÍCH SHAP VALUES ===\n• person_income: -1.9118 (TIÊU CỰC (-))\n• loan_intent: -1.7893 (TIÊU CỰC (-))\n• person_age: -0.9083 (TIÊU CỰC (-))\n• person_home_ownership: -0.6261 (TIÊU CỰC (-))\n• cb_person_default_on_file: -0.5715 (TIÊU CỰC (-))\n• loan_amnt: +0.4310 (TÍCH CỰC (+))\n• cb_person_cred_hist_length: -0.0488 (TIÊU CỰC (-))\n• person_emp_length: +0.0459 (TÍCH CỰC (+))"},
# ]
# inputs = tokenizer.apply_chat_template(
#     messages,
#     add_generation_prompt = False,
#     return_tensors = "pt",
#     return_dict = True,
#     reasoning_effort = "low",
# ).to(model.device)
# from transformers import TextStreamer
# _ = model.generate(**inputs, max_new_tokens = 1024, streamer = TextStreamer(tokenizer))

!pip install fastapi uvicorn

from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI()

# Định nghĩa request schema
class Query(BaseModel):
    messages: list

@app.post("/generate")
def generate_text(query: Query):
    # Chuẩn bị input từ client gửi
    inputs = tokenizer.apply_chat_template(
        query.messages,
        add_generation_prompt=False,
        return_tensors="pt",
        return_dict=True,
    ).to(model.device)

    # Sinh text
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
    )

    # Decode text
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"generated_text": result}

!pip install pyngrok

!ngrok config add-authtoken 330hXNioJpmIoVOjUPnNtBcHgf6_3V5sjZpnF5tGHMmGzt4oD

from pyngrok import ngrok
public_url = ngrok.connect(8000)
print(public_url)

from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn, threading

# Load tokenizer HF chuẩn (có apply_chat_template)
tokenizer = AutoTokenizer.from_pretrained(model_name)

app = FastAPI()

class Query(BaseModel):
    messages: list

@app.post("/generate")
def generate_text(query: Query):
    inputs = tokenizer.apply_chat_template(
        query.messages,
        add_generation_prompt=False,
        return_tensors="pt",
        return_dict=True,
    ).to(model.device)

    outputs = model.generate(**inputs, max_new_tokens=512)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"generated_text": result}

# Chạy server trong background
def run():
    uvicorn.run(app, host="0.0.0.0", port=8000)

threading.Thread(target=run).start()

def run():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Mở ngrok tunnel
public_url = ngrok.connect(8000)
print("Public URL:", public_url)

# Start FastAPI trong background
threading.Thread(target=run).start()